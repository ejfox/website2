1 Text, 2 Text
"What is forecast calibration?","Forecasts match reality at stated probabilities. Events given 70% probability happen ~70% of the time. Overconfident: 90% predictions right only 75%. Underconfident: 60% predictions right 80%."
"How do you measure calibration?","Group forecasts by probability bin (60-70%, 70-80%, etc.). Calculate fraction that came true in each bin. Plot forecast probability vs observed frequency. Perfect calibration = 45-degree line."
"What is a reliability diagram?","Calibration plot: x-axis = forecast probability, y-axis = observed frequency. Points above diagonal = underconfident. Below diagonal = overconfident. Visual check of calibration."
"What is the Brier score?","Mean of (forecast - outcome)² where outcome is 0 or 1. Lower = better. Brier = 0 is perfect. Brier = 0.25 is random guessing. Rewards both calibration and sharpness (confident correct predictions)."
"What is sharpness in forecasting?","Making confident predictions (high or low probability, not 50%). Perfect calibration at 50-50 every time is useless. Good forecasters are sharp AND calibrated. Trade-off: more sharpness risks miscalibration."
"What is the calibration-sharpness tradeoff?","Can always be calibrated by predicting base rates. But that's not useful. Want to be calibrated while also being informative. Push for sharpness until calibration breaks."
"What is resolution in forecasting?","Ability to distinguish outcomes that will happen from those that won't. High resolution = predictions vary meaningfully. Brier score decomposes into: reliability (calibration) + resolution - uncertainty."
"How do you communicate probability to non-experts?","Avoid naked numbers ('70% chance'). Use comparisons: 'about as likely as drawing a red card.' Show uncertainty visually. Emphasize what could go wrong. Discuss scenarios, not just point forecast."
"What is 'wet street' communication error?","Confusing P(wet street | rain) with P(rain | wet street). In elections: saying 'Clinton has 70% chance' heard as 'Clinton will win.' Need to emphasize uncertainty is real—30% is NOT unlikely."
"How did 2016 expose probability communication failures?","538's 70% Clinton forecast was reasonable, but many readers heard 'certain.' Media focused on polls, not uncertainty. 'Trump can't win' narrative despite genuine uncertainty. Need better uncertainty communication."
"What is a fan chart for uncertainty?","Visualization showing central forecast with expanding confidence bands over time. Wider bands = more uncertainty. Shows range of plausible outcomes. Better than single point forecast."
"What is scenario communication?","Instead of just '70% Dem wins,' describe: 'If polls are right, Dem wins comfortably. If 2016-style error, very close. If 2020-style error, narrow Dem win.' Makes uncertainty concrete."
"How should you handle being wrong?","Distinguish bad luck from bad process. 30% events happen 30% of the time—that's not failure. Check calibration across many forecasts. If systematically wrong, fix model. Document and learn."
"What is Tetlock's 'superforecaster' concept?","Some forecasters are consistently more accurate. Traits: probabilistic thinking, update with evidence, avoid overconfidence, seek diverse information, track record. Can be trained."
"What does 'I don't know' mean in forecasting?","Honest uncertainty is valuable. If you can't do better than 50-50, say so. False precision is worse than admitting ignorance. Forecasters should know limits of their knowledge."
