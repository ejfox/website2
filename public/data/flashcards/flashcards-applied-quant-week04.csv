1 Text, 2 Text
"What does a standard error measure?","Uncertainty about an estimate. SE tells you how much the coefficient would vary if you repeated the study many times. Smaller SE = more precise estimate. SE shrinks as sample size grows (SE ∝ 1/√n)."
"How do you interpret a 95% confidence interval?","If we repeated this study many times, 95% of the calculated intervals would contain the true value. NOT '95% probability true value is in this interval.' The true value is fixed; the interval is random."
"What does a p-value actually mean?","Probability of seeing results this extreme (or more) IF the null hypothesis were true. p=0.03 means: if there were truly no effect, you'd see data this extreme only 3% of the time. It's about the data, not the hypothesis."
"What are the ASA's key warnings about p-values?","(1) P-values don't measure probability hypothesis is true. (2) P-values don't measure effect size or importance. (3) p<0.05 is arbitrary threshold. (4) Don't base conclusions solely on p-values. (5) Context and prior evidence matter."
"What's the difference between statistical and practical significance?","Statistical: effect is unlikely to be zero (small p-value). Practical: effect is large enough to matter. A huge study can detect tiny, meaningless effects. Always report effect SIZE with confidence interval, not just p-value."
"What is the significance filter problem?","Published studies only show significant results. If true effect = 0.5, among studies finding significance, average estimate is ~1.2 (biased high). Significant results systematically overestimate true effects. This is publication bias."
"How does sample size affect inference?","Larger n → smaller SE → narrower CI → more power to detect effects. With n=20, might not detect real effect; with n=20,000, can detect tiny irrelevant effects. Power analysis helps choose appropriate n."
"What is statistical power?","Probability of detecting an effect when one truly exists. Power = 1 - P(Type II error). Typical target: 80%. Low power means you might miss real effects. Depends on sample size, effect size, and alpha level."
"What is Type I vs Type II error?","Type I: reject null when it's true (false positive). Type II: fail to reject null when it's false (false negative). α controls Type I rate (usually 0.05). Power = 1 - Type II rate."
"Why is 'the coefficient is significant' incomplete?","Need to report: (1) coefficient value (the effect size), (2) standard error or CI (the uncertainty), (3) whether effect is practically meaningful in context. p<0.05 alone tells you almost nothing useful."
"What does 'statistically indistinguishable from zero' mean?","The confidence interval includes zero. We can't rule out no effect with our data. NOT 'the effect is zero'—could be non-zero but our study lacked power to detect it. Absence of evidence ≠ evidence of absence."
"How do you assess if a study had sufficient power?","Calculate: What effect size could this study reliably detect? If minimum detectable effect is larger than what's practically meaningful, study is underpowered. Many published studies are underpowered."
"What's the problem with multiple comparisons?","If you test 20 hypotheses at α=0.05, expect 1 false positive by chance. 'P-hacking' = testing many things and reporting only significant ones. Need correction (Bonferroni) or pre-registration of hypotheses."
"What is a one-sided vs two-sided test?","One-sided: tests if effect is specifically positive OR negative. Two-sided: tests if effect is non-zero in either direction. Two-sided is more conservative (harder to get significance) and usually more honest."
