1 Text, 2 Text
"What is a sample survey in polling context?","Interview subset of population to learn about the whole. Random sample allows inference to population. But: who responds? Who's missed? Response rate matters. Modern response rates often <10%."
"What is sampling error vs non-sampling error?","Sampling: random variation from surveying subset instead of everyone. ∝ 1/√n. Non-sampling: bias from who responds, how questions asked, mode effects. Non-sampling error usually bigger problem."
"What is a likely voter screen?","Filter to identify respondents who will actually vote. Based on past voting, stated intention, registration. Different screens give different results. LV samples are smaller, more uncertain than RV (registered voters)."
"What is a house effect?","Systematic bias of a pollster relative to average. Pollster X might consistently show Democrats +2 vs average. Caused by sampling, weighting, LV screen differences. Can adjust for in aggregation."
"What is weighting in polls?","Adjusting sample to match population demographics. If sample is 40% college grads but population is 30%, downweight college responses. But: what if education correlates with response propensity AND vote?"
"Why did 2016 and 2020 polls underestimate Trump?","Non-response bias: certain Trump voters less likely to respond to polls. Education weighting helped but didn't fully fix. 'Shy Trump' theory debated. Social trust may affect who talks to pollsters."
"What is poll aggregation?","Combining multiple polls for better estimate. Simple average, weighted average (by sample size, recency, quality), or model-based. Reduces random error. But systematic errors don't average away."
"How do you weight polls by recency?","Older polls get less weight: w_t = λ^(days_old). λ < 1 discounts past. Higher λ = longer memory. Typical: half-life of ~7 days. Tension: stability vs responsiveness to change."
"What is pollster quality/rating?","FiveThirtyEight rates pollsters A+ to F based on historical accuracy. Higher-rated polls get more weight in aggregation. Based on methodology transparency, sample size, past performance."
"Why is polling error correlated across polls?","Polls face similar biases: same non-responders, similar methods. If one poll is off by +3 Dem, others likely off similarly. Can't diversify away by averaging. Correlated errors → higher forecast uncertainty."
"What is margin of error in polling?","Usually 95% CI for the estimate. Poll of 1000 has ~3 point MOE: if poll shows 52%, true could be 49-55%. But MOE only covers sampling error, not non-response bias or other issues."
"How does sample size affect poll precision?","MOE ∝ 1/√n. n=400: MOE ≈ 5 points. n=1600: MOE ≈ 2.5 points. Diminishing returns. Going from 1000 to 4000 only halves MOE. Most gain from first few hundred respondents."
"What is herding in polling?","Pollsters adjust results to match other polls, reducing apparent spread. Leads to overconfident consensus. If all polls converge, less information than if they varied. Watch for suspiciously tight clustering."
"What is poll averaging vs election forecasting?","Poll average: what polls say right now. Forecast: prediction of election day outcome. Forecast adds: likely voter adjustments, fundamentals, trend projection, uncertainty modeling."
"What data sources exist for historical polling?","FiveThirtyEight (free, downloadable). RealClearPolitics. MIT Election Lab (historical). ANES (survey methodology research). Roper Center (academic archive)."
