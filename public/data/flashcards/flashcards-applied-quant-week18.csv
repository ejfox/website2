1 Text, 2 Text
"Why publish forecasts publicly?","Accountability forces honesty. Can't quietly revise history. Builds track record. Contributes to public understanding. Forces you to think carefully about what you believe. Skin in the game."
"What should a methodology document include?","Data sources, model structure, key assumptions, what's excluded, uncertainty quantification, known limitations, comparison to alternatives, update schedule. Reader should be able to critique."
"What visualizations work for election forecasts?","Win probability (big number or gauge). Margin distribution (histogram or density). Uncertainty cone over time. Map of race ratings. Scenario outcomes. Avoid pie charts; favor number lines."
"What is a race rating (Lean/Likely/Safe)?","Categorical assessment: Safe D/R (>95%), Likely (75-95%), Lean (55-75%), Toss-up (<55%). Simpler than probabilities. But hides information—two 'Lean D' races can have very different probabilities."
"When should you update your forecast?","New data (polls, fundamentals). Model improvements (fix bugs). Major events (candidate change, scandal). But avoid over-updating to noise. Balance responsiveness with stability."
"What is the snake chart/race ranking?","Races ordered by competitiveness, from most R to most D. Shows tipping point (218th seat). Width = uncertainty. Used by 538 and others. Good for seeing path to majority."
"How do you handle election night updates?","Pre-plan: what results would change your forecast? As counties report, update in real time. Compare actual vs expected. Publish updates with timestamp. Don't panic-update on partial data."
"What's the difference between forecast and projection?","Forecast: prediction of what will happen. Projection: what would happen under certain assumptions. 'If turnout is X, Dem wins.' Projections explore scenarios; forecasts commit to prediction."
"What should you NOT do in forecast publication?","Cherry-pick favorable comparisons. Hide assumptions. Claim false precision. Update without noting changes. Blame bad luck without examining process. Disappear after being wrong."
"What is pre-registration for forecasts?","Committing to model and methodology before seeing outcome. Prevents post-hoc rationalization. Document: 'This is my model as of [date], before election.' Compare against actual outcome honestly."
"How do you evaluate your forecast after the election?","Calculate Brier score. Check calibration. Compare to benchmarks (polls-only, 538, betting markets). Identify biggest misses. Ask: was this bad luck or bad process? Document lessons."
"What role do prediction markets play?","Aggregate dispersed information via betting. Polymarket, PredictIt, Kalshi. Historically accurate but with biases. Compare your forecast to markets—persistent disagreement needs justification."
"What ethical considerations exist in election forecasting?","Could forecasts affect turnout? (Probably minimal.) Could they be weaponized? Responsible: focus on process, not outcome. Don't claim certainty. Acknowledge limitations."
"What makes a forecast 'honest'?","States assumptions explicitly. Quantifies uncertainty realistically. Acknowledges limitations. Doesn't hide embarrassing aspects. Updates transparently. Evaluates fairly afterward."
