{
  "name": "openrouter-census",
  "description": "ü§ñ Complete analysis of 316+ AI models on OpenRouter with interactive Observable Plot dashboard",
  "url": "https://github.com/ejfox/openrouter-census",
  "homepage": null,
  "stats": {
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "openIssues": 0
  },
  "language": "JavaScript",
  "languageColor": "#f1e05a",
  "languages": {
    "JavaScript": 296195,
    "HTML": 146774,
    "Shell": 581
  },
  "diskUsage": 135,
  "topics": [],
  "readme": {
    "html": "<h1 id=\"-openrouter-model-census\">ü§ñ OpenRouter Model Census</h1>\n<p>A complete analysis tool for <strong>all 316+ AI models</strong> on OpenRouter‚Äîfeaturing fast scraping, Python-style data analysis, and beautiful Observable Plot visualizations. Get pricing insights, provider comparisons, and model capabilities in an interactive dashboard.</p>\n<p><img src=\"https://img.shields.io/badge/Charts-7%20Interactive-blue\" alt=\"Dashboard Preview\" /> <img src=\"https://img.shields.io/badge/Models-316+-green\" alt=\"Models\" /> <img src=\"https://img.shields.io/badge/Providers-54+-orange\" alt=\"Providers\" /></p>\n<hr />\n<h2 id=\"-quick-start\">üöÄ Quick Start</h2>\n<pre><code class=\"language-bash\"># Clone and install\ngit clone https://github.com/your-username/openrouter-scrape\ncd openrouter-scrape\nnpm install\n\n# Run complete pipeline (scrape + analyze + serve dashboard)\nnpm run full-pipeline\n</code></pre>\n<p><strong>That's it!</strong> Your dashboard opens at <code>http://localhost:8080</code> with fresh data on all 316+ models.</p>\n<h2 id=\"-features\">‚ú® Features</h2>\n<ul>\n<li><strong>‚ö° Fast scraping</strong> - All 316 models in under 1 second</li>\n<li><strong>üìä Beautiful charts</strong> - 7 interactive Observable Plot visualizations</li>\n<li><strong>üí∞ Pricing insights</strong> - Find the cheapest models, price vs context analysis</li>\n<li><strong>üè¢ Provider analysis</strong> - Market share, strategy mapping, capabilities</li>\n<li><strong>üì± Responsive design</strong> - Works great on mobile and desktop</li>\n<li><strong>üîÑ Easy updates</strong> - Re-run anytime for fresh data</li>\n</ul>\n<hr />\n<h2 id=\"what-gets-collected\">What gets collected</h2>\n<h3 id=\"layer-1--models-get-apiv1models\">Layer 1 ‚Äî Models (<code>GET /api/v1/models</code>)</h3>\n<ul>\n<li>Identity: <code>id</code>, <code>name</code>, <code>canonical_slug</code>, <code>created</code>, <code>description</code></li>\n<li>Architecture: <code>input_modalities</code>, <code>output_modalities</code>, <code>tokenizer</code>, <code>instruct_type</code></li>\n<li>Aggregates: <code>context_length</code> (model-wide), <code>supported_parameters</code> (<strong>union across providers</strong>)</li>\n<li>Pricing (as published): <code>prompt</code>, <code>completion</code>, <code>request</code>, <code>image</code>, <code>web_search</code>, <code>internal_reasoning</code>, <code>input_cache_read</code>, <code>input_cache_write</code></li>\n<li>Top provider summary: <code>context_length</code>, <code>max_completion_tokens</code>, <code>is_moderated</code>\nNotes: <code>supported_parameters</code> at model level is a <strong>union</strong>, not guaranteed at any single endpoint. (<a href=\"https://openrouter.ai/docs/models?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n</ul>\n<h3 id=\"layer-2--provider-endpoints-get-apiv1modelsauthorslugendpoints\">Layer 2 ‚Äî Provider Endpoints (<code>GET /api/v1/models/:author/:slug/endpoints</code>)</h3>\n<p>Per provider that hosts the model:</p>\n<ul>\n<li><code>provider_name</code>, <code>name</code> (endpoint), <code>status</code>, <code>uptime_last_30m</code></li>\n<li>Limits: <code>context_length</code>, <code>max_prompt_tokens</code>, <code>max_completion_tokens</code>, <code>quantization</code></li>\n<li>Capabilities: endpoint-level <code>supported_parameters</code></li>\n<li>Pricing overrides: <code>prompt</code>, <code>completion</code>, <code>request</code>, <code>image</code>\nUse these to resolve real availability and price for your route. (<a href=\"https://openrouter.ai/docs/api-reference/list-endpoints-for-a-model?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n</ul>\n<h3 id=\"layer-3--optional-generation-telemetry-get-apiv1generation-or-inline-usage-accounting\">Layer 3 ‚Äî Optional Generation Telemetry (<code>GET /api/v1/generation</code> or inline <strong>usage accounting</strong>)</h3>\n<p>If you have generation IDs or enable usage accounting in your workloads:</p>\n<ul>\n<li>Identity &amp; routing: <code>model</code>, <code>provider_name</code>, <code>origin</code>, <code>upstream_id</code></li>\n<li>Tokens (native): <code>native_tokens_prompt</code>, <code>native_tokens_completion</code>, <code>native_tokens_reasoning</code></li>\n<li>Tokens (gateway): <code>tokens_prompt</code>, <code>tokens_completion</code></li>\n<li>Cost: <code>total_cost</code>, <code>cache_discount</code>, <code>upstream_inference_cost</code></li>\n<li>Timings: <code>latency</code>, <code>generation_time</code>, moderation latency</li>\n<li>Termination: <code>finish_reason</code>, <code>native_finish_reason</code></li>\n<li>Media/search counters; cached token counts (reads)\nThis enables empirical latency/cost analyses grounded in the provider‚Äôs native tokenizer. (<a href=\"https://openrouter.ai/docs/api-reference/overview?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n</ul>\n<hr />\n<h2 id=\"outputs-immutable-analysis-ready\">Outputs (immutable, analysis-ready)</h2>\n<ul>\n<li>\n<p><code>artifacts/YYYY-MM-DD/</code></p>\n<ul>\n<li><code>models.jsonl</code> (one model per line; raw API objects)</li>\n<li><code>endpoints.jsonl</code> (provider-expanded rows)</li>\n<li><code>joined.parquet</code> (denormalized: model ‚®Ø endpoint)</li>\n<li><code>telemetry.jsonl</code> (optional generation/usage rows)</li>\n<li><code>schema/*.json</code> (JSON Schema for each file)</li>\n<li><code>MANIFEST.sha256</code> (file ‚Üí SHA-256)</li>\n<li><code>FETCH_METADATA.json</code> (request URLs, headers, etags, timestamps, versions)</li>\n<li><code>CITATION.cff</code> (how to cite this snapshot)</li>\n<li><code>NOTICE.txt</code> (license + terms provenance)</li>\n</ul>\n</li>\n<li>\n<p>All files compressed with <code>zstd</code> if <code>--zstd</code> is set.</p>\n</li>\n</ul>\n<hr />\n<h2 id=\"install\">Install</h2>\n<pre><code class=\"language-bash\"># Node 20+ recommended\npnpm i -g openrouter-census   # or: npx openrouter-census\n</code></pre>\n<p>Local dev:</p>\n<pre><code class=\"language-bash\">git clone https://github.com/yourorg/openrouter-census\ncd openrouter-census\npnpm install\n</code></pre>\n<hr />\n<h2 id=\"configuration\">Configuration</h2>\n<pre><code class=\"language-bash\"># Optional: required only if you fetch protected usage for your own generations\nexport OPENROUTER_API_KEY=... \n\n# Rate limits (Bottleneck): default is conservative; override as needed\nexport CENSUS_RESERVOIR=100        # max requests per window\nexport CENSUS_INTERVAL_MS=60_000   # window length\nexport CENSUS_MAX_CONCURRENCY=8    # parallelism\n</code></pre>\n<blockquote>\n<p>Tip: tune Bottleneck‚Äôs <strong>reservoir</strong> + <strong>interval</strong> for burst/steady control. Keep headroom for Cloudflare/DDOS protections and any per-account limits. (<a href=\"https://openrouter.ai/docs/api-reference/limits?utm_source=chatgpt.com\">OpenRouter</a>, <a href=\"https://dev.to/arifszn/prevent-api-overload-a-comprehensive-guide-to-rate-limiting-with-bottleneck-c2p?utm_source=chatgpt.com\">DEV Community</a>)</p>\n</blockquote>\n<hr />\n<h2 id=\"usage-cli--tui\">Usage (CLI &amp; TUI)</h2>\n<pre><code class=\"language-bash\">openrouter-census tui\n</code></pre>\n<p><strong>TUI features</strong></p>\n<ul>\n<li>Live counters: queued / inflight / completed / failed</li>\n<li>Throttles: adjust concurrency &amp; reservoir at runtime</li>\n<li>Filters: by provider, family, modality, supports(param)</li>\n<li>Export panel: write JSONL/Parquet + schemas + manifest</li>\n<li>Retry controls: backoff/rehydrate failed requests</li>\n</ul>\n<p><strong>Non-interactive</strong></p>\n<pre><code class=\"language-bash\"># Full scrape ‚Üí immutable snapshot directory\nopenrouter-census run --out artifacts/$(date +%F) --zstd\n\n# Fetch with telemetry (ids from stdin) and merge\ncat ids.txt | openrouter-census usage --append artifacts/2025-08-21/telemetry.jsonl\n</code></pre>\n<hr />\n<h2 id=\"data-model-schemas\">Data model (schemas)</h2>\n<ul>\n<li><code>schema/models.schema.json</code> ‚Äì exact API fields from <code>/models</code> with permissive <code>additionalProperties: true</code></li>\n<li><code>schema/endpoints.schema.json</code> ‚Äì rows keyed by <code>(model_id, provider_name, endpoint_name)</code></li>\n<li><code>schema/telemetry.schema.json</code> ‚Äì union of <code>/generation</code> + inline usage accounting fields</li>\n<li><code>schema/joined.schema.json</code> ‚Äì normalized and namespaced (<code>model.*</code>, <code>endpoint.*</code>, <code>price.*</code>, <code>limits.*</code>)</li>\n</ul>\n<p>Schemas are versioned; each run stores the <strong>exact</strong> schema copy used to validate outputs.</p>\n<hr />\n<h2 id=\"reproducibility--provenance\">Reproducibility &amp; provenance</h2>\n<ul>\n<li><strong>Deterministic traversal:</strong> stable sort of model IDs; endpoint requests queued in lexical order.</li>\n<li><strong>Version pins:</strong> <code>pnpm-lock.yaml</code> checked in; run logs emit Node + package versions.</li>\n<li><strong>Raw preservation:</strong> we persist <strong>unmodified</strong> API payloads alongside normalized tables.</li>\n<li><strong>Integrity:</strong> SHA-256 digests and <code>FETCH_METADATA.json</code> (timestamps, etags, cache status).</li>\n<li><strong>Citations:</strong> <code>CITATION.cff</code> includes title, version, DOI placeholder, and snapshot date.</li>\n<li><strong>License/Terms capture:</strong> <code>NOTICE.txt</code> records the doc URLs and access dates for API docs.</li>\n</ul>\n<hr />\n<h2 id=\"limits-caveats-ethics\">Limits, caveats, ethics</h2>\n<ul>\n<li><code>supported_parameters</code> at model level is <strong>union</strong>; intersect with endpoint capabilities for truth in UX or routing guarantees. (<a href=\"https://openrouter.ai/docs/api-reference/list-available-models?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>Endpoint <code>uptime_last_30m</code> is a short pulse, not an SLA. Prefer aggregates if you collect telemetry. (<a href=\"https://openrouter.ai/docs/api-reference/list-endpoints-for-a-model?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>Pricing may differ per endpoint; do not assume model-level pricing applies to your chosen provider. (<a href=\"https://openrouter.ai/docs/api-reference/list-endpoints-for-a-model?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>Respect rate limits &amp; terms; don‚Äôt stress shared free endpoints. Consider backoffs and caching. (<a href=\"https://openrouter.ai/docs/api-reference/limits?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n</ul>\n<hr />\n<h2 id=\"programmatic-api-library-usage\">Programmatic API (library usage)</h2>\n<pre><code class=\"language-ts\">import { census } from \"openrouter-census\";\n\nconst run = await census({\n  outDir: \"artifacts/2025-08-21\",\n  concurrency: 8,\n  reservoir: 100,\n  intervalMs: 60_000,\n  fetchTelemetry: false,\n});\nconsole.log(run.summary);\n</code></pre>\n<hr />\n<h2 id=\"minimal-pipeline-spec\">Minimal pipeline spec</h2>\n<ol>\n<li>\n<p><strong>Models pass</strong></p>\n<ul>\n<li>GET <code>https://openrouter.ai/api/v1/models</code> ‚Üí write raw ‚Üí validate ‚Üí extract rows. (<a href=\"https://openrouter.ai/docs/models?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n</ul>\n</li>\n<li>\n<p><strong>Endpoints pass</strong></p>\n<ul>\n<li>For each <code>{author}/{slug}</code>, GET <code>/api/v1/models/:author/:slug/endpoints</code> ‚Üí write raw ‚Üí validate ‚Üí rows. (<a href=\"https://openrouter.ai/docs/api-reference/list-endpoints-for-a-model?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n</ul>\n</li>\n<li>\n<p><strong>(Optional) Telemetry pass</strong></p>\n<ul>\n<li>For provided generation IDs or inline usage payloads, normalize and append to <code>telemetry.jsonl</code>. (<a href=\"https://openrouter.ai/docs/api-reference/overview?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n</ul>\n</li>\n<li>\n<p><strong>Join &amp; export</strong></p>\n<ul>\n<li>Create <code>joined.parquet</code> + schemas + manifest + checksums.</li>\n</ul>\n</li>\n</ol>\n<hr />\n<h2 id=\"what-you-can-visualize-later-suggestions\">What you can visualize later (suggestions)</h2>\n<ul>\n<li><strong>Price vs context</strong> (per endpoint)</li>\n<li><strong>Parameter support heatmap</strong> (models √ó params; intersected, not union)</li>\n<li><strong>Short-window reliability</strong> (endpoint uptime)</li>\n<li><strong>Observed latency &amp; effective $/1k</strong> from telemetry (native tokens)</li>\n</ul>\n<hr />\n<h2 id=\"references\">References</h2>\n<ul>\n<li>Models API &amp; schema. (<a href=\"https://openrouter.ai/docs/models?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>List models (union of <code>supported_parameters</code>). (<a href=\"https://openrouter.ai/docs/api-reference/list-available-models?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>Per-model endpoints (provider-specific limits/pricing/uptime). (<a href=\"https://openrouter.ai/docs/api-reference/list-endpoints-for-a-model?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>Usage accounting (native tokens, cached tokens, reasoning tokens). (<a href=\"https://openrouter.ai/docs/use-cases/usage-accounting?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>Precise token accounting &amp; <code>/generation</code>. (<a href=\"https://openrouter.ai/docs/api-reference/overview?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>Reasoning tokens overview. (<a href=\"https://openrouter.ai/docs/use-cases/reasoning-tokens?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>OpenRouter limits (guidance for throttling). (<a href=\"https://openrouter.ai/docs/api-reference/limits?utm_source=chatgpt.com\">OpenRouter</a>)</li>\n<li>Bottleneck patterns for Node rate limiting. (<a href=\"https://dev.to/arifszn/prevent-api-overload-a-comprehensive-guide-to-rate-limiting-with-bottleneck-c2p?utm_source=chatgpt.com\">DEV Community</a>)</li>\n</ul>\n<hr />\n<h2 id=\"license\">License</h2>\n<p>MIT for code. Data artifacts inherit upstream terms; see <code>NOTICE.txt</code>.</p>",
    "raw": "# ü§ñ OpenRouter Model Census\n\nA complete analysis tool for **all 316+ AI models** on OpenRouter‚Äîfeaturing fast scraping, Python-style data analysis, and beautiful Observable Plot visualizations. Get pricing insights, provider comparisons, and model capabilities in an interactive dashboard.\n\n![Dashboard Preview](https://img.shields.io/badge/Charts-7%20Interactive-blue) ![Models](https://img.shields.io/badge/Models-316+-green) ![Providers](https://img.shields.io/badge/Providers-54+-orange)\n\n---\n\n## üöÄ Quick Start\n\n```bash\n# Clone and install\ngit clone https://github.com/your-username/openrouter-scrape\ncd openrouter-scrape\nnpm install\n\n# Run complete pipeline (scrape + analyze + serve dashboard)\nnpm run full-pipeline\n```\n\n**That's it!** Your dashboard opens at `http://localhost:8080` with fresh data on all 316+ models.\n\n## ‚ú® Features\n\n* **‚ö° Fast scraping** - All 316 models in under 1 second\n* **üìä Beautiful charts** - 7 interactive Observable Plot visualizations  \n* **üí∞ Pricing insights** - Find the cheapest models, price vs context analysis\n* **üè¢ Provider analysis** - Market share, strategy mapping, capabilities\n* **üì± Responsive design** - Works great on mobile and desktop\n* **üîÑ Easy updates** - Re-run anytime for fresh data\n\n---\n\n## What gets collected\n\n### Layer 1 ‚Äî Models (`GET /api/v1/models`)\n\n* Identity: `id`, `name`, `canonical_slug`, `created`, `description`\n* Architecture: `input_modalities`, `output_modalities`, `tokenizer`, `instruct_type`\n* Aggregates: `context_length` (model-wide), `supported_parameters` (**union across providers**)\n* Pricing (as published): `prompt`, `completion`, `request`, `image`, `web_search`, `internal_reasoning`, `input_cache_read`, `input_cache_write`\n* Top provider summary: `context_length`, `max_completion_tokens`, `is_moderated`\n  Notes: `supported_parameters` at model level is a **union**, not guaranteed at any single endpoint. ([OpenRouter][2])\n\n### Layer 2 ‚Äî Provider Endpoints (`GET /api/v1/models/:author/:slug/endpoints`)\n\nPer provider that hosts the model:\n\n* `provider_name`, `name` (endpoint), `status`, `uptime_last_30m`\n* Limits: `context_length`, `max_prompt_tokens`, `max_completion_tokens`, `quantization`\n* Capabilities: endpoint-level `supported_parameters`\n* Pricing overrides: `prompt`, `completion`, `request`, `image`\n  Use these to resolve real availability and price for your route. ([OpenRouter][4])\n\n### Layer 3 ‚Äî Optional Generation Telemetry (`GET /api/v1/generation` or inline **usage accounting**)\n\nIf you have generation IDs or enable usage accounting in your workloads:\n\n* Identity & routing: `model`, `provider_name`, `origin`, `upstream_id`\n* Tokens (native): `native_tokens_prompt`, `native_tokens_completion`, `native_tokens_reasoning`\n* Tokens (gateway): `tokens_prompt`, `tokens_completion`\n* Cost: `total_cost`, `cache_discount`, `upstream_inference_cost`\n* Timings: `latency`, `generation_time`, moderation latency\n* Termination: `finish_reason`, `native_finish_reason`\n* Media/search counters; cached token counts (reads)\n  This enables empirical latency/cost analyses grounded in the provider‚Äôs native tokenizer. ([OpenRouter][3])\n\n---\n\n## Outputs (immutable, analysis-ready)\n\n* `artifacts/YYYY-MM-DD/`\n\n  * `models.jsonl` (one model per line; raw API objects)\n  * `endpoints.jsonl` (provider-expanded rows)\n  * `joined.parquet` (denormalized: model ‚®Ø endpoint)\n  * `telemetry.jsonl` (optional generation/usage rows)\n  * `schema/*.json` (JSON Schema for each file)\n  * `MANIFEST.sha256` (file ‚Üí SHA-256)\n  * `FETCH_METADATA.json` (request URLs, headers, etags, timestamps, versions)\n  * `CITATION.cff` (how to cite this snapshot)\n  * `NOTICE.txt` (license + terms provenance)\n* All files compressed with `zstd` if `--zstd` is set.\n\n---\n\n## Install\n\n```bash\n# Node 20+ recommended\npnpm i -g openrouter-census   # or: npx openrouter-census\n```\n\nLocal dev:\n\n```bash\ngit clone https://github.com/yourorg/openrouter-census\ncd openrouter-census\npnpm install\n```\n\n---\n\n## Configuration\n\n```bash\n# Optional: required only if you fetch protected usage for your own generations\nexport OPENROUTER_API_KEY=... \n\n# Rate limits (Bottleneck): default is conservative; override as needed\nexport CENSUS_RESERVOIR=100        # max requests per window\nexport CENSUS_INTERVAL_MS=60_000   # window length\nexport CENSUS_MAX_CONCURRENCY=8    # parallelism\n```\n\n> Tip: tune Bottleneck‚Äôs **reservoir** + **interval** for burst/steady control. Keep headroom for Cloudflare/DDOS protections and any per-account limits. ([OpenRouter][5], [DEV Community][6])\n\n---\n\n## Usage (CLI & TUI)\n\n```bash\nopenrouter-census tui\n```\n\n**TUI features**\n\n* Live counters: queued / inflight / completed / failed\n* Throttles: adjust concurrency & reservoir at runtime\n* Filters: by provider, family, modality, supports(param)\n* Export panel: write JSONL/Parquet + schemas + manifest\n* Retry controls: backoff/rehydrate failed requests\n\n**Non-interactive**\n\n```bash\n# Full scrape ‚Üí immutable snapshot directory\nopenrouter-census run --out artifacts/$(date +%F) --zstd\n\n# Fetch with telemetry (ids from stdin) and merge\ncat ids.txt | openrouter-census usage --append artifacts/2025-08-21/telemetry.jsonl\n```\n\n---\n\n## Data model (schemas)\n\n* `schema/models.schema.json` ‚Äì exact API fields from `/models` with permissive `additionalProperties: true`\n* `schema/endpoints.schema.json` ‚Äì rows keyed by `(model_id, provider_name, endpoint_name)`\n* `schema/telemetry.schema.json` ‚Äì union of `/generation` + inline usage accounting fields\n* `schema/joined.schema.json` ‚Äì normalized and namespaced (`model.*`, `endpoint.*`, `price.*`, `limits.*`)\n\nSchemas are versioned; each run stores the **exact** schema copy used to validate outputs.\n\n---\n\n## Reproducibility & provenance\n\n* **Deterministic traversal:** stable sort of model IDs; endpoint requests queued in lexical order.\n* **Version pins:** `pnpm-lock.yaml` checked in; run logs emit Node + package versions.\n* **Raw preservation:** we persist **unmodified** API payloads alongside normalized tables.\n* **Integrity:** SHA-256 digests and `FETCH_METADATA.json` (timestamps, etags, cache status).\n* **Citations:** `CITATION.cff` includes title, version, DOI placeholder, and snapshot date.\n* **License/Terms capture:** `NOTICE.txt` records the doc URLs and access dates for API docs.\n\n---\n\n## Limits, caveats, ethics\n\n* `supported_parameters` at model level is **union**; intersect with endpoint capabilities for truth in UX or routing guarantees. ([OpenRouter][1])\n* Endpoint `uptime_last_30m` is a short pulse, not an SLA. Prefer aggregates if you collect telemetry. ([OpenRouter][4])\n* Pricing may differ per endpoint; do not assume model-level pricing applies to your chosen provider. ([OpenRouter][4])\n* Respect rate limits & terms; don‚Äôt stress shared free endpoints. Consider backoffs and caching. ([OpenRouter][5])\n\n---\n\n## Programmatic API (library usage)\n\n```ts\nimport { census } from \"openrouter-census\";\n\nconst run = await census({\n  outDir: \"artifacts/2025-08-21\",\n  concurrency: 8,\n  reservoir: 100,\n  intervalMs: 60_000,\n  fetchTelemetry: false,\n});\nconsole.log(run.summary);\n```\n\n---\n\n## Minimal pipeline spec\n\n1. **Models pass**\n\n   * GET `https://openrouter.ai/api/v1/models` ‚Üí write raw ‚Üí validate ‚Üí extract rows. ([OpenRouter][2])\n2. **Endpoints pass**\n\n   * For each `{author}/{slug}`, GET `/api/v1/models/:author/:slug/endpoints` ‚Üí write raw ‚Üí validate ‚Üí rows. ([OpenRouter][4])\n3. **(Optional) Telemetry pass**\n\n   * For provided generation IDs or inline usage payloads, normalize and append to `telemetry.jsonl`. ([OpenRouter][3])\n4. **Join & export**\n\n   * Create `joined.parquet` + schemas + manifest + checksums.\n\n---\n\n## What you can visualize later (suggestions)\n\n* **Price vs context** (per endpoint)\n* **Parameter support heatmap** (models √ó params; intersected, not union)\n* **Short-window reliability** (endpoint uptime)\n* **Observed latency & effective \\$/1k** from telemetry (native tokens)\n\n---\n\n## References\n\n* Models API & schema. ([OpenRouter][2])\n* List models (union of `supported_parameters`). ([OpenRouter][1])\n* Per-model endpoints (provider-specific limits/pricing/uptime). ([OpenRouter][4])\n* Usage accounting (native tokens, cached tokens, reasoning tokens). ([OpenRouter][7])\n* Precise token accounting & `/generation`. ([OpenRouter][3])\n* Reasoning tokens overview. ([OpenRouter][8])\n* OpenRouter limits (guidance for throttling). ([OpenRouter][5])\n* Bottleneck patterns for Node rate limiting. ([DEV Community][6])\n\n---\n\n## License\n\nMIT for code. Data artifacts inherit upstream terms; see `NOTICE.txt`.\n\n[1]: https://openrouter.ai/docs/api-reference/list-available-models?utm_source=chatgpt.com \"List available models | OpenRouter | Documentation\"\n[2]: https://openrouter.ai/docs/models?utm_source=chatgpt.com \"Access 400+ AI Models Through One API - OpenRouter\"\n[3]: https://openrouter.ai/docs/api-reference/overview?utm_source=chatgpt.com \"OpenRouter API Reference | Complete API Documentation\"\n[4]: https://openrouter.ai/docs/api-reference/list-endpoints-for-a-model?utm_source=chatgpt.com \"List endpoints for a model | OpenRouter | Documentation\"\n[5]: https://openrouter.ai/docs/api-reference/limits?utm_source=chatgpt.com \"API Rate Limits | Configure Usage Limits in OpenRouter\"\n[6]: https://dev.to/arifszn/prevent-api-overload-a-comprehensive-guide-to-rate-limiting-with-bottleneck-c2p?utm_source=chatgpt.com \"Prevent API Overload: A Comprehensive Guide to Rate Limiting with ...\"\n[7]: https://openrouter.ai/docs/use-cases/usage-accounting?utm_source=chatgpt.com \"Usage Accounting | Track AI Model Usage with OpenRouter\"\n[8]: https://openrouter.ai/docs/use-cases/reasoning-tokens?utm_source=chatgpt.com \"Reasoning Tokens | Enhanced AI Model Reasoning with OpenRouter\"\n",
    "excerpt": "A complete analysis tool for all 316+ AI models on OpenRouter‚Äîfeaturing fast scraping, Python-style data analysis, and beautiful Observable Plot visualizations...."
  },
  "createdAt": "2025-08-21T18:25:17Z",
  "updatedAt": "2025-08-21T18:25:25Z",
  "pushedAt": "2025-08-21T18:25:21Z"
}