{
  "name": "vizhub-benchmarks",
  "description": "AI code editing benchmarks for interactive visuals",
  "url": "https://github.com/vizhub-core/vizhub-benchmarks",
  "homepage": null,
  "stats": {
    "stars": 0,
    "forks": 0,
    "watchers": 1,
    "openIssues": 5
  },
  "language": "HTML",
  "languageColor": "#e34c26",
  "languages": {},
  "diskUsage": 1045,
  "topics": [],
  "readme": {
    "html": "<h1 id=\"vizhub-benchmarks\">vizhub-benchmarks</h1>\n<p>AI code editing benchmarks for interactive visuals + <strong>training dataset generation</strong></p>\n<h2 id=\"overview\">Overview</h2>\n<p>This project serves dual strategic purposes:</p>\n<ol>\n<li><strong>Model Evaluation</strong>: Systematic assessment of AI coding capabilities across models (GPT, Claude, Llama, DeepSeek, etc.)</li>\n<li><strong>Training Data Generation</strong>: High-quality human-rated datasets for LLM fine-tuning and research</li>\n</ol>\n<h2 id=\"quick-start\">Quick Start</h2>\n<pre><code class=\"language-bash\"># Run benchmarks to generate code samples\nnpm run benchmark\n\n# Launch collaborative grading interface  \nnpm run grade\n\n# Export training dataset in RLHF format\nnpm run export:huggingface\n</code></pre>\n<h2 id=\"benchmark-system\">Benchmark System</h2>\n<h3 id=\"running-benchmarks\">Running Benchmarks</h3>\n<p>Test how different AI models perform on code tasks:</p>\n<pre><code class=\"language-bash\"># Run all benchmarks with default models\nnpm run benchmark\n\n# Run a specific challenge\nnpm run benchmark -- --challenge stockPriceChart\n\n# Specify which models to test\nnpm run benchmark -- --models gpt-4,claude-3\n\n# Enable caching for faster development\nnpm run benchmark -- --cache\n</code></pre>\n<h3 id=\"using-the-grader-ui\">Using the Grader UI</h3>\n<p>The benchmark system includes a grader for evaluating AI-generated visualizations:</p>\n<pre><code class=\"language-bash\"># Launch the grader UI\nnpm run grade\n\n# Focus on a specific challenge\nnpm run grade -- --challenge stockPriceChart\n</code></pre>\n<h4 id=\"grading-workflow\">Grading Workflow</h4>\n<ol>\n<li><strong>Select Challenge</strong>: Choose from available challenges in the dropdown</li>\n<li><strong>Browse Models</strong>: Navigate between different AI models' solutions</li>\n<li><strong>Review Visualization</strong>: See the rendered visualization and screenshot</li>\n<li><strong>Inspect Code</strong>: Review the generated code</li>\n<li><strong>Assign Scores</strong>:\n<ul>\n<li><strong>Functionality (0-5)</strong>: How well it meets requirements</li>\n<li><strong>Aesthetics (0-5)</strong>: Visual appeal and usability</li>\n</ul>\n</li>\n<li><strong>Add Notes</strong>: Provide specific feedback</li>\n<li><strong>Submit Grade</strong>: Save evaluation to the results database</li>\n</ol>\n<h4 id=\"scoring-guidelines\">Scoring Guidelines</h4>\n<p><strong>Functionality (0-5)</strong>:</p>\n<ul>\n<li>0: Does not work</li>\n<li>1: Major bugs</li>\n<li>2: Works but missing requirements</li>\n<li>3: Meets basic requirements</li>\n<li>4: Implements all requirements well</li>\n<li>5: Perfect implementation with extras</li>\n</ul>\n<p><strong>Aesthetics (0-5)</strong>:</p>\n<ul>\n<li>0: Unusable layout</li>\n<li>1: Poor design</li>\n<li>2: Basic appearance</li>\n<li>3: Clean design</li>\n<li>4: Well-designed with good UX</li>\n<li>5: Exceptional design</li>\n</ul>\n<h2 id=\"collaborative-grading-system\">Collaborative Grading System</h2>\n<h3 id=\"current-state-vs-ideal-structure\">Current State vs. Ideal Structure</h3>\n<p><strong>Current Issues:</strong></p>\n<ul>\n<li>Grader saves results to <code>grader-app/public/benchmarks/results/results.csv</code> (isolated)</li>\n<li>No git integration for collaborative contributions</li>\n<li>Manual file copying between main repo and grader app</li>\n<li>Results aren't versioned or shared between graders</li>\n</ul>\n<p><strong>Ideal Collaborative Structure:</strong></p>\n<pre><code>benchmarks/\nâ”œâ”€â”€ results/\nâ”‚   â”œâ”€â”€ results.csv              # Main results file (git-tracked)\nâ”‚   â”œâ”€â”€ grades/                  # Individual grader contributions\nâ”‚   â”‚   â”œâ”€â”€ alice-2024-01-15.csv # Timestamped grader files\nâ”‚   â”‚   â”œâ”€â”€ bob-2024-01-16.csv\nâ”‚   â”‚   â””â”€â”€ claire-2024-01-17.csv\nâ”‚   â””â”€â”€ consensus/               # Aggregated consensus grades\nâ”‚       â””â”€â”€ consensus.csv        # Merged/averaged results\nâ”œâ”€â”€ challenges/                  # Challenge implementations (existing)\nâ””â”€â”€ visualizations/             # Generated outputs (existing)\n</code></pre>\n<h3 id=\"planned-collaborative-workflow\">Planned Collaborative Workflow</h3>\n<ol>\n<li>\n<p><strong>Individual Grading</strong>:</p>\n<ul>\n<li>Each grader works on a local copy</li>\n<li>Grader saves to timestamped file: <code>grades/{grader-name}-{date}.csv</code></li>\n<li>Grader commits their individual grades to git</li>\n<li>Creates PR with their grading session</li>\n</ul>\n</li>\n<li>\n<p><strong>Grade Aggregation</strong>:</p>\n<ul>\n<li>Automated script merges individual grades</li>\n<li>Handles conflicts (multiple grades for same result)</li>\n<li>Generates consensus scores (median/average)</li>\n<li>Updates main <code>results.csv</code> with consensus</li>\n</ul>\n</li>\n<li>\n<p><strong>Git Integration</strong>:</p>\n<ul>\n<li>Each grading session = git commit</li>\n<li>Grader identity tracked in commit metadata</li>\n<li>Full audit trail of grading decisions</li>\n<li>Easy diffing between grading sessions</li>\n</ul>\n</li>\n<li>\n<p><strong>Quality Assurance</strong>:</p>\n<ul>\n<li>Flag results with high grade variance</li>\n<li>Track inter-grader reliability</li>\n<li>Identify results needing re-evaluation</li>\n<li>Generate grading statistics and reports</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"technical-implementation-plan\">Technical Implementation Plan</h3>\n<p><strong>Phase 1: File Structure</strong></p>\n<ul>\n<li>Create <code>benchmarks/results/grades/</code> directory</li>\n<li>Modify grader to save individual grade files</li>\n<li>Update file paths and data flow</li>\n</ul>\n<p><strong>Phase 2: Git Integration</strong></p>\n<ul>\n<li>Auto-commit individual grading sessions</li>\n<li>Generate meaningful commit messages</li>\n<li>Add grader metadata to commits</li>\n</ul>\n<p><strong>Phase 3: Aggregation System</strong></p>\n<ul>\n<li>Script to merge individual grades</li>\n<li>Consensus calculation algorithms</li>\n<li>Conflict resolution strategies</li>\n</ul>\n<p><strong>Phase 4: Quality Tools</strong></p>\n<ul>\n<li>Inter-grader agreement metrics</li>\n<li>Grade variance analysis</li>\n<li>Automated quality reports</li>\n</ul>\n<h3 id=\"benefits\">Benefits</h3>\n<ul>\n<li><strong>Distributed Grading</strong>: Multiple people can contribute grades independently</li>\n<li><strong>Version Control</strong>: Full history of grading decisions</li>\n<li><strong>Quality Control</strong>: Statistical analysis of grader agreement</li>\n<li><strong>Transparency</strong>: Open process with audit trail</li>\n<li><strong>Scalability</strong>: Easy to add new graders and challenges</li>\n</ul>\n<h2 id=\"-training-dataset-generation\">ðŸ¤– Training Dataset Generation</h2>\n<h3 id=\"huggingface-rlhf-export\">HuggingFace RLHF Export</h3>\n<p>Transform human evaluations into industry-standard training datasets:</p>\n<pre><code class=\"language-bash\"># Export to standard RLHF format\nnpm run export:huggingface\n\n# Custom export options\nnpm run export:huggingface -- --output my-dataset.jsonl --min-votes 2 --verbose\n</code></pre>\n<p><strong>Output</strong>: <code>{prompt, chosen, rejected}</code> triplets compatible with:</p>\n<ul>\n<li>OpenAI fine-tuning API</li>\n<li>HuggingFace TRL (Transformer Reinforcement Learning)</li>\n<li>Anthropic Constitutional AI</li>\n<li>Academic research pipelines</li>\n</ul>\n<h3 id=\"dataset-statistics\">Dataset Statistics</h3>\n<p>Current export generates:</p>\n<ul>\n<li><strong>9 training examples</strong> from existing evaluations</li>\n<li><strong>10 models</strong> across major AI providers</li>\n<li><strong>Consensus scoring</strong> with confidence intervals</li>\n<li><strong>Full metadata</strong> for reproducible research</li>\n</ul>\n<h3 id=\"research-applications\">Research Applications</h3>\n<p><strong>Model Training</strong>:</p>\n<ul>\n<li>Fine-tune coding models with human preference data</li>\n<li>Train reward models for automated code assessment</li>\n<li>Enable RLHF (Reinforcement Learning from Human Feedback)</li>\n</ul>\n<p><strong>Academic Research</strong>:</p>\n<ul>\n<li>Benchmark datasets with standardized methodology</li>\n<li>Reproducible evaluation frameworks</li>\n<li>Multi-dimensional quality assessment</li>\n</ul>\n<p><strong>Industry Applications</strong>:</p>\n<ul>\n<li>Model selection based on human-validated performance</li>\n<li>Code quality assessment tools</li>\n<li>Training data for specialized domains</li>\n</ul>\n<p>See <a href=\"scripts/README.md\"><code>scripts/README.md</code></a> for detailed technical documentation.</p>",
    "raw": "# vizhub-benchmarks\nAI code editing benchmarks for interactive visuals + **training dataset generation**\n\n## Overview\n\nThis project serves dual strategic purposes:\n\n1. **Model Evaluation**: Systematic assessment of AI coding capabilities across models (GPT, Claude, Llama, DeepSeek, etc.)\n2. **Training Data Generation**: High-quality human-rated datasets for LLM fine-tuning and research\n\n## Quick Start\n\n```bash\n# Run benchmarks to generate code samples\nnpm run benchmark\n\n# Launch collaborative grading interface  \nnpm run grade\n\n# Export training dataset in RLHF format\nnpm run export:huggingface\n```\n\n## Benchmark System\n\n### Running Benchmarks\n\nTest how different AI models perform on code tasks:\n\n```bash\n# Run all benchmarks with default models\nnpm run benchmark\n\n# Run a specific challenge\nnpm run benchmark -- --challenge stockPriceChart\n\n# Specify which models to test\nnpm run benchmark -- --models gpt-4,claude-3\n\n# Enable caching for faster development\nnpm run benchmark -- --cache\n```\n\n\n\n\n### Using the Grader UI\n\nThe benchmark system includes a grader for evaluating AI-generated visualizations:\n\n```bash\n# Launch the grader UI\nnpm run grade\n\n# Focus on a specific challenge\nnpm run grade -- --challenge stockPriceChart\n```\n\n#### Grading Workflow\n\n1. **Select Challenge**: Choose from available challenges in the dropdown\n2. **Browse Models**: Navigate between different AI models' solutions\n3. **Review Visualization**: See the rendered visualization and screenshot\n4. **Inspect Code**: Review the generated code\n5. **Assign Scores**:\n   - **Functionality (0-5)**: How well it meets requirements\n   - **Aesthetics (0-5)**: Visual appeal and usability\n6. **Add Notes**: Provide specific feedback\n7. **Submit Grade**: Save evaluation to the results database\n\n#### Scoring Guidelines\n\n**Functionality (0-5)**:\n\n- 0: Does not work\n- 1: Major bugs\n- 2: Works but missing requirements\n- 3: Meets basic requirements\n- 4: Implements all requirements well\n- 5: Perfect implementation with extras\n\n**Aesthetics (0-5)**:\n\n- 0: Unusable layout\n- 1: Poor design\n- 2: Basic appearance\n- 3: Clean design\n- 4: Well-designed with good UX\n- 5: Exceptional design\n\n## Collaborative Grading System\n\n### Current State vs. Ideal Structure\n\n**Current Issues:**\n- Grader saves results to `grader-app/public/benchmarks/results/results.csv` (isolated)\n- No git integration for collaborative contributions\n- Manual file copying between main repo and grader app\n- Results aren't versioned or shared between graders\n\n**Ideal Collaborative Structure:**\n\n```\nbenchmarks/\nâ”œâ”€â”€ results/\nâ”‚   â”œâ”€â”€ results.csv              # Main results file (git-tracked)\nâ”‚   â”œâ”€â”€ grades/                  # Individual grader contributions\nâ”‚   â”‚   â”œâ”€â”€ alice-2024-01-15.csv # Timestamped grader files\nâ”‚   â”‚   â”œâ”€â”€ bob-2024-01-16.csv\nâ”‚   â”‚   â””â”€â”€ claire-2024-01-17.csv\nâ”‚   â””â”€â”€ consensus/               # Aggregated consensus grades\nâ”‚       â””â”€â”€ consensus.csv        # Merged/averaged results\nâ”œâ”€â”€ challenges/                  # Challenge implementations (existing)\nâ””â”€â”€ visualizations/             # Generated outputs (existing)\n```\n\n### Planned Collaborative Workflow\n\n1. **Individual Grading**:\n   - Each grader works on a local copy\n   - Grader saves to timestamped file: `grades/{grader-name}-{date}.csv`\n   - Grader commits their individual grades to git\n   - Creates PR with their grading session\n\n2. **Grade Aggregation**:\n   - Automated script merges individual grades\n   - Handles conflicts (multiple grades for same result)\n   - Generates consensus scores (median/average)\n   - Updates main `results.csv` with consensus\n\n3. **Git Integration**:\n   - Each grading session = git commit\n   - Grader identity tracked in commit metadata\n   - Full audit trail of grading decisions\n   - Easy diffing between grading sessions\n\n4. **Quality Assurance**:\n   - Flag results with high grade variance\n   - Track inter-grader reliability\n   - Identify results needing re-evaluation\n   - Generate grading statistics and reports\n\n### Technical Implementation Plan\n\n**Phase 1: File Structure**\n- Create `benchmarks/results/grades/` directory\n- Modify grader to save individual grade files\n- Update file paths and data flow\n\n**Phase 2: Git Integration**\n- Auto-commit individual grading sessions\n- Generate meaningful commit messages\n- Add grader metadata to commits\n\n**Phase 3: Aggregation System**\n- Script to merge individual grades\n- Consensus calculation algorithms\n- Conflict resolution strategies\n\n**Phase 4: Quality Tools**\n- Inter-grader agreement metrics\n- Grade variance analysis\n- Automated quality reports\n\n### Benefits\n\n- **Distributed Grading**: Multiple people can contribute grades independently\n- **Version Control**: Full history of grading decisions\n- **Quality Control**: Statistical analysis of grader agreement\n- **Transparency**: Open process with audit trail\n- **Scalability**: Easy to add new graders and challenges\n\n## ðŸ¤– Training Dataset Generation\n\n### HuggingFace RLHF Export\n\nTransform human evaluations into industry-standard training datasets:\n\n```bash\n# Export to standard RLHF format\nnpm run export:huggingface\n\n# Custom export options\nnpm run export:huggingface -- --output my-dataset.jsonl --min-votes 2 --verbose\n```\n\n**Output**: `{prompt, chosen, rejected}` triplets compatible with:\n- OpenAI fine-tuning API\n- HuggingFace TRL (Transformer Reinforcement Learning)\n- Anthropic Constitutional AI\n- Academic research pipelines\n\n### Dataset Statistics\n\nCurrent export generates:\n- **9 training examples** from existing evaluations\n- **10 models** across major AI providers\n- **Consensus scoring** with confidence intervals\n- **Full metadata** for reproducible research\n\n### Research Applications\n\n**Model Training**:\n- Fine-tune coding models with human preference data\n- Train reward models for automated code assessment\n- Enable RLHF (Reinforcement Learning from Human Feedback)\n\n**Academic Research**:\n- Benchmark datasets with standardized methodology\n- Reproducible evaluation frameworks\n- Multi-dimensional quality assessment\n\n**Industry Applications**:\n- Model selection based on human-validated performance\n- Code quality assessment tools\n- Training data for specialized domains\n\nSee [`scripts/README.md`](scripts/README.md) for detailed technical documentation.\n",
    "excerpt": "AI code editing benchmarks for interactive visuals + training dataset generation\n\n\n\nThis project serves dual strategic purposes:\n\n1. Model Evaluation: Systemati..."
  },
  "createdAt": "2025-04-06T14:10:42Z",
  "updatedAt": "2025-09-20T11:17:45Z",
  "pushedAt": "2025-09-20T11:17:41Z"
}