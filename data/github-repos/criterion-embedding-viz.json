{
  "name": "criterion-embedding-viz",
  "description": "Generate embeddings from every Criterion movie",
  "url": "https://github.com/ejfox/criterion-embedding-viz",
  "homepage": null,
  "stats": {
    "stars": 1,
    "forks": 0,
    "watchers": 1,
    "openIssues": 5
  },
  "language": "JavaScript",
  "languageColor": "#f1e05a",
  "topics": [],
  "readme": {
    "html": "<h1 id=\"criterion-embedding-visualization\">Criterion Embedding Visualization</h1>\n<img width=\"1012\" alt=\"Screenshot 2024-12-01 at 4 54 29 PM\" src=\"https://github.com/user-attachments/assets/59b4f762-3dd5-4c46-bd45-98bdff8f0535\">\n<p>This project is designed to create vector embeddings for Criterion movie titles and descriptions using the Nomic Embedding API. The embeddings can be used for advanced data analysis, clustering, and visualization, enabling deeper exploration of the Criterion Channel's catalog.</p>\n<h2 id=\"note-about-embeddings-file\">Note About Embeddings File</h2>\n<p>The <code>criterion_embeddings.json</code> file (~156MB) is stored on R2 and not in the Git repository due to its large size. Use the provided download script to fetch it.</p>\n<h2 id=\"objectives\">Objectives</h2>\n<p>The primary objective of this project is to leverage natural language processing (NLP) techniques to generate meaningful embeddings for textual data in the Criterion movie dataset. These embeddings encode semantic relationships between movie titles and descriptions, which can be used in tasks such as similarity analysis, clustering, and visualization.</p>\n<h2 id=\"provenance\">Provenance</h2>\n<p>The dataset utilized in this project originates from a publicly available spreadsheet shared on Reddit by <a href=\"https://www.reddit.com/user/morbusiff\">u/morbusiff</a>. The spreadsheet contains detailed information on movies available on the Criterion Channel as of 2019.</p>\n<ul>\n<li><strong>Source Spreadsheet</strong>: <a href=\"https://docs.google.com/spreadsheets/d/1-ctl5IGVUqfkCH48DFUbLx0iQai9r6BLG9NStMwxPSw/edit?gid=740795620#gid=740795620\">Criterion Channel Videos Spreadsheet</a></li>\n<li><strong>Original Reddit Post</strong>: <a href=\"https://www.reddit.com/r/criterion/comments/bba5go/4176_criterion_channel_videos_in_a_spreadsheet/\">4,176 Criterion Channel Videos in a Spreadsheet</a></li>\n</ul>\n<p>We acknowledge and thank <a href=\"https://www.reddit.com/user/morbusiff\">u/morbusiff</a> for compiling and sharing this valuable dataset.</p>\n<h2 id=\"methodology\">Methodology</h2>\n<ol>\n<li>\n<p><strong>Data Input</strong>:</p>\n<ul>\n<li>The dataset is provided in CSV format (<code>criterion_movies.csv</code>) and contains information such as titles, descriptions, directors, years, and links.</li>\n</ul>\n</li>\n<li>\n<p><strong>Embedding Generation</strong>:</p>\n<ul>\n<li>The <code>index.js</code> script processes the dataset to generate embeddings using the <a href=\"https://docs.nomic.ai/\">Nomic Embedding API</a>.</li>\n<li>Separate embeddings are created for both the <strong>title</strong> and <strong>description</strong> of each movie to capture different semantic representations.</li>\n</ul>\n</li>\n<li>\n<p><strong>Batch Processing</strong>:</p>\n<ul>\n<li>The script processes data in batches to optimize API usage.</li>\n<li>Rate-limiting is implemented via the <code>bottleneck</code> library to respect API constraints.</li>\n</ul>\n</li>\n<li>\n<p><strong>Output</strong>:</p>\n<ul>\n<li>Embeddings are saved in JSON format (<code>criterion_embeddings.json</code>), maintaining a structured representation of the data alongside the generated embeddings.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"features\">Features</h2>\n<ul>\n<li><strong>Efficient Batch Processing</strong>: Groups multiple embeddings in a single API call to reduce overhead.</li>\n<li><strong>Title and Description Embeddings</strong>: Provides separate embeddings for both fields to allow fine-grained analysis.</li>\n<li><strong>Progress Saving and Resumption</strong>: Automatically resumes processing from the last completed batch after interruptions.</li>\n<li><strong>Rate Limiting</strong>: Ensures compliance with API constraints using <code>bottleneck</code>.</li>\n</ul>\n<h2 id=\"requirements\">Requirements</h2>\n<ul>\n<li>Node.js (v14 or higher)</li>\n<li>A valid Nomic API key</li>\n</ul>\n<h2 id=\"installation\">Installation</h2>\n<ol>\n<li>\n<p>Clone the repository:</p>\n<pre><code class=\"language-bash\">git clone https://github.com/ejfox/criterion-embedding-viz.git\r\ncd criterion-embedding-viz\n</code></pre>\n</li>\n<li>\n<p>Install dependencies:</p>\n<pre><code class=\"language-bash\">npm install\n</code></pre>\n</li>\n<li>\n<p>Configure environment variables by creating a <code>.env</code> file:</p>\n<pre><code class=\"language-bash\">echo \"NOMIC_API_KEY=your_nomic_api_key\" > .env\n</code></pre>\n<h3 id=\"advanced-configuration-options\">Advanced Configuration Options</h3>\n<pre><code class=\"language-bash\"># Output format\r\nOUTPUT_FORMAT=ndjson        # \"json\" or \"ndjson\" (newline-delimited JSON)\r\nOUTPUT_FILE=criterion_embeddings.ndjson\r\n\r\n# Embedding configuration\r\nTASK_TYPE=search_document   # \"search_document\", \"search_query\", \"clustering\", \"classification\"\r\nDIMENSIONALITY=768          # 768 or 256 for Nomic\n</code></pre>\n</li>\n<li>\n<p>Place your dataset in the root directory as <code>criterion_movies.csv</code>.</p>\n</li>\n<li>\n<p>Download the embeddings file:</p>\n<pre><code class=\"language-bash\">./download-embeddings.sh\n</code></pre>\n<p>This downloads the pre-generated embeddings file (~156MB) from Cloudflare R2.</p>\n</li>\n</ol>\n<h2 id=\"execution\">Execution</h2>\n<p>Run the script to generate embeddings:</p>\n<pre><code class=\"language-bash\">node index.js\n</code></pre>\n<h2 id=\"data-output\">Data Output</h2>\n<p>The script generates embeddings in <code>criterion_embeddings.json</code>. Each entry includes:</p>\n<ul>\n<li>Metadata from the CSV dataset.</li>\n<li>Separate embeddings for the movie title and description.</li>\n</ul>\n<h3 id=\"sample-json-output\">Sample JSON Output</h3>\n<pre><code class=\"language-json\">[\r\n  {\r\n    \"Title (Data retrieved 2019-06-21)\": \"Mulholland Dr.\",\r\n    \"Description\": \"Directed by David Lynch...\",\r\n    \"title_embedding\": [0.0256958, 0.00015819073, ...],\r\n    \"description_embedding\": [0.03456134, -0.0124586, ...]\r\n  },\r\n  ...\r\n]\n</code></pre>\n<h2 id=\"applications\">Applications</h2>\n<p>The generated embeddings can be used for:</p>\n<ul>\n<li>Semantic similarity analysis between movies.</li>\n<li>Clustering based on descriptive content.</li>\n<li>Visualization of relationships within the dataset using dimensionality reduction techniques (e.g., PCA, t-SNE, UMAP).</li>\n</ul>\n<h2 id=\"limitations\">Limitations</h2>\n<ul>\n<li>The embeddings are limited to the semantic information provided in titles and descriptions. Additional metadata (e.g., genre, director) could enhance future analyses.</li>\n<li>Generated embeddings are dependent on the Nomic API's embedding model as of the time of execution.</li>\n</ul>\n<h2 id=\"ethical-considerations\">Ethical Considerations</h2>\n<ul>\n<li><strong>Data Provenance</strong>: The dataset was shared publicly and is used for analytical purposes. Attribution is provided to the original compiler.</li>\n<li><strong>Intellectual Property</strong>: Ensure proper use of Criterion Channel data in compliance with its terms of service and copyright regulations.</li>\n</ul>\n<h2 id=\"future-enhancements--todo\">Future Enhancements / TODO</h2>\n<h3 id=\"wikipedia-enrichment\">Wikipedia Enrichment</h3>\n<p>Concept: Automatically find and embed Wikipedia articles for each movie to create richer embeddings:</p>\n<ul>\n<li>Use Wikipedia API to search for each movie title + year + director</li>\n<li>Implement human spot-checking interface to verify correct matches</li>\n<li>Extract and chunk Wikipedia content by logical sections (plot, cast, production, reception, etc.)</li>\n<li>Generate embeddings for each section separately</li>\n<li>Could enable deeper semantic search like \"films about existentialism\" or \"movies with troubled productions\"</li>\n<li>Store Wikipedia URLs and section embeddings alongside movie data</li>\n</ul>\n<h3 id=\"multi-provider-embedding-support\">Multi-Provider Embedding Support</h3>\n<p>Make it easy to swap between different embedding services:</p>\n<ul>\n<li><strong>OpenRouter</strong> (priority) - Access to multiple models through one API</li>\n<li>OpenAI embeddings (text-embedding-3-small/large)</li>\n<li>Cohere embeddings</li>\n<li>Local embeddings (sentence-transformers)</li>\n</ul>\n<p>Challenges to solve:</p>\n<ul>\n<li>Different providers use different dimensions (OpenAI: 1536/3072, Nomic: 768/256, etc.)</li>\n<li>Need abstraction layer to handle different API formats</li>\n<li>Store provider metadata with embeddings for compatibility</li>\n<li>Consider dimension reduction techniques for cross-provider compatibility</li>\n</ul>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p>Special thanks to <a href=\"https://www.reddit.com/user/morbusiff\">u/morbusiff</a> for compiling and sharing the original dataset on Reddit.</p>",
    "raw": "# Criterion Embedding Visualization\r\n\r\n<img width=\"1012\" alt=\"Screenshot 2024-12-01 at 4 54 29 PM\" src=\"https://github.com/user-attachments/assets/59b4f762-3dd5-4c46-bd45-98bdff8f0535\">\r\n\r\nThis project is designed to create vector embeddings for Criterion movie titles and descriptions using the Nomic Embedding API. The embeddings can be used for advanced data analysis, clustering, and visualization, enabling deeper exploration of the Criterion Channel's catalog.\r\n\r\n## Note About Embeddings File\r\n\r\nThe `criterion_embeddings.json` file (~156MB) is stored on R2 and not in the Git repository due to its large size. Use the provided download script to fetch it.\r\n\r\n## Objectives\r\n\r\nThe primary objective of this project is to leverage natural language processing (NLP) techniques to generate meaningful embeddings for textual data in the Criterion movie dataset. These embeddings encode semantic relationships between movie titles and descriptions, which can be used in tasks such as similarity analysis, clustering, and visualization.\r\n\r\n## Provenance\r\n\r\nThe dataset utilized in this project originates from a publicly available spreadsheet shared on Reddit by [u/morbusiff](https://www.reddit.com/user/morbusiff). The spreadsheet contains detailed information on movies available on the Criterion Channel as of 2019. \r\n\r\n- **Source Spreadsheet**: [Criterion Channel Videos Spreadsheet](https://docs.google.com/spreadsheets/d/1-ctl5IGVUqfkCH48DFUbLx0iQai9r6BLG9NStMwxPSw/edit?gid=740795620#gid=740795620)\r\n- **Original Reddit Post**: [4,176 Criterion Channel Videos in a Spreadsheet](https://www.reddit.com/r/criterion/comments/bba5go/4176_criterion_channel_videos_in_a_spreadsheet/)\r\n\r\nWe acknowledge and thank [u/morbusiff](https://www.reddit.com/user/morbusiff) for compiling and sharing this valuable dataset.\r\n\r\n## Methodology\r\n\r\n1. **Data Input**:\r\n   - The dataset is provided in CSV format (`criterion_movies.csv`) and contains information such as titles, descriptions, directors, years, and links.\r\n\r\n2. **Embedding Generation**:\r\n   - The `index.js` script processes the dataset to generate embeddings using the [Nomic Embedding API](https://docs.nomic.ai/).\r\n   - Separate embeddings are created for both the **title** and **description** of each movie to capture different semantic representations.\r\n\r\n3. **Batch Processing**:\r\n   - The script processes data in batches to optimize API usage.\r\n   - Rate-limiting is implemented via the `bottleneck` library to respect API constraints.\r\n\r\n4. **Output**:\r\n   - Embeddings are saved in JSON format (`criterion_embeddings.json`), maintaining a structured representation of the data alongside the generated embeddings.\r\n\r\n## Features\r\n\r\n- **Efficient Batch Processing**: Groups multiple embeddings in a single API call to reduce overhead.\r\n- **Title and Description Embeddings**: Provides separate embeddings for both fields to allow fine-grained analysis.\r\n- **Progress Saving and Resumption**: Automatically resumes processing from the last completed batch after interruptions.\r\n- **Rate Limiting**: Ensures compliance with API constraints using `bottleneck`.\r\n\r\n## Requirements\r\n\r\n- Node.js (v14 or higher)\r\n- A valid Nomic API key\r\n\r\n## Installation\r\n\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/ejfox/criterion-embedding-viz.git\r\n   cd criterion-embedding-viz\r\n   ```\r\n\r\n2. Install dependencies:\r\n   ```bash\r\n   npm install\r\n   ```\r\n\r\n3. Configure environment variables by creating a `.env` file:\r\n   ```bash\r\n   echo \"NOMIC_API_KEY=your_nomic_api_key\" > .env\r\n   ```\r\n   \r\n   ### Advanced Configuration Options\r\n   ```bash\r\n   # Output format\r\n   OUTPUT_FORMAT=ndjson        # \"json\" or \"ndjson\" (newline-delimited JSON)\r\n   OUTPUT_FILE=criterion_embeddings.ndjson\r\n   \r\n   # Embedding configuration\r\n   TASK_TYPE=search_document   # \"search_document\", \"search_query\", \"clustering\", \"classification\"\r\n   DIMENSIONALITY=768          # 768 or 256 for Nomic\r\n   ```\r\n\r\n4. Place your dataset in the root directory as `criterion_movies.csv`.\r\n\r\n5. Download the embeddings file:\r\n   ```bash\r\n   ./download-embeddings.sh\r\n   ```\r\n   This downloads the pre-generated embeddings file (~156MB) from Cloudflare R2.\r\n\r\n## Execution\r\n\r\nRun the script to generate embeddings:\r\n```bash\r\nnode index.js\r\n```\r\n\r\n## Data Output\r\n\r\nThe script generates embeddings in `criterion_embeddings.json`. Each entry includes:\r\n- Metadata from the CSV dataset.\r\n- Separate embeddings for the movie title and description.\r\n\r\n### Sample JSON Output\r\n```json\r\n[\r\n  {\r\n    \"Title (Data retrieved 2019-06-21)\": \"Mulholland Dr.\",\r\n    \"Description\": \"Directed by David Lynch...\",\r\n    \"title_embedding\": [0.0256958, 0.00015819073, ...],\r\n    \"description_embedding\": [0.03456134, -0.0124586, ...]\r\n  },\r\n  ...\r\n]\r\n```\r\n\r\n## Applications\r\n\r\nThe generated embeddings can be used for:\r\n- Semantic similarity analysis between movies.\r\n- Clustering based on descriptive content.\r\n- Visualization of relationships within the dataset using dimensionality reduction techniques (e.g., PCA, t-SNE, UMAP).\r\n\r\n## Limitations\r\n\r\n- The embeddings are limited to the semantic information provided in titles and descriptions. Additional metadata (e.g., genre, director) could enhance future analyses.\r\n- Generated embeddings are dependent on the Nomic API's embedding model as of the time of execution.\r\n\r\n## Ethical Considerations\r\n\r\n- **Data Provenance**: The dataset was shared publicly and is used for analytical purposes. Attribution is provided to the original compiler.\r\n- **Intellectual Property**: Ensure proper use of Criterion Channel data in compliance with its terms of service and copyright regulations.\r\n\r\n## Future Enhancements / TODO\r\n\r\n### Wikipedia Enrichment\r\nConcept: Automatically find and embed Wikipedia articles for each movie to create richer embeddings:\r\n- Use Wikipedia API to search for each movie title + year + director\r\n- Implement human spot-checking interface to verify correct matches\r\n- Extract and chunk Wikipedia content by logical sections (plot, cast, production, reception, etc.)\r\n- Generate embeddings for each section separately\r\n- Could enable deeper semantic search like \"films about existentialism\" or \"movies with troubled productions\"\r\n- Store Wikipedia URLs and section embeddings alongside movie data\r\n\r\n### Multi-Provider Embedding Support\r\nMake it easy to swap between different embedding services:\r\n- **OpenRouter** (priority) - Access to multiple models through one API\r\n- OpenAI embeddings (text-embedding-3-small/large)\r\n- Cohere embeddings\r\n- Local embeddings (sentence-transformers)\r\n\r\nChallenges to solve:\r\n- Different providers use different dimensions (OpenAI: 1536/3072, Nomic: 768/256, etc.)\r\n- Need abstraction layer to handle different API formats\r\n- Store provider metadata with embeddings for compatibility\r\n- Consider dimension reduction techniques for cross-provider compatibility\r\n\r\n## Acknowledgments\r\n\r\nSpecial thanks to [u/morbusiff](https://www.reddit.com/user/morbusiff) for compiling and sharing the original dataset on Reddit.\r\n\r\n\r\n",
    "excerpt": "<img width=\"1012\" alt=\"Screenshot 2024-12-01 at 4 54 29 PM\" src=\"https://github.com/user-attachments/assets/59b4f762-3dd5-4c46-bd45-98bdff8f0535\">\r\n\r\nThis proje..."
  },
  "createdAt": "2024-12-01T21:50:43Z",
  "updatedAt": "2025-06-20T18:55:53Z",
  "pushedAt": "2025-06-20T20:13:23Z"
}